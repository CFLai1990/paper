\section{Discussion}
\label{section:discussion}
\note{
\col{Outline for discussion:
\begin{enumerate}[(1)]
\item Extension: other featured projections, non-linear projections
\item Impact of data scale: increased dimensionality or data size, try a larger dataset with higher dimensionality
\item Comparison: local-preserving algorithms, projections for classification
\item Problems: lack of navigation, overlapping dimensions in a projection
\end{enumerate}
}
}
In this work, we propose to customize linear projections to facilitate analyses regarding the user-specified local POI data. In Section~\ref{section:casestudy}, we prove the effectiveness of our method in revealing hidden relationships and reducing local distortions. However, there are still flaws in the proposed method, which we would like to discuss in this section. After that, we illustrate the relationships and differences between our method and some state of art machine learning techniques. At last, we show that our method can be extended into a more general methodology.

The first flaw we'd like to mention, is the lack of effective navigation in the exploration. Dimension weights are like the steering wheel. They provide good controls to fine-tune a projection, though the tuning is often aimless. In our method, the steering wheel is the focus with its features. The exploration inevitably jumps between diverse projections when the focus or metric is updated. Even supplied with animations, users may still get confused when facing with the abrupt changes. Besides, we do not reserve historical images unless they are added in the focus list. Some intermediate results could be lost if they are missed by mistake. A proper way to remedy this flaw, is to provide dimensional controls as well as the navigation history.

The second flaw is about the dimensional analysis. At present, we interpret the projected data by perceiving their distribution along the projected axes. However, such interpretation is not very precise, especially when the projected directions interfere with each other. Besides, dimension weights will be generally lower when dimensionality increases. The projected axes will be too dense to discern. Nevertheless, the subspace suggestion still works in higher-dimensional cases. For example, when analyzing a facial images dataset, our method can reveal a subspace where two sets of faces are most different. In other words, it shows the pixels that are most different between two faces. This can help to interpret the data, reduce data dimensions and design classification models.

In fact, in the machine learning field, the commonly studied data use to have huge amounts of dimensions, while each dimension alone (e.g. a pixel) is not so meaningful. We usually assume the data to be placed on a low-dimensional manifold, where neighborhood relationships are more important than dimensional semantics. In this work, we also seek to preserve data locality, but we decide to maintain dimensional context by using linear projections. That's because, in a more general case, it's of interest what factors could largely affect the data distribution. It's quite an interesting finding if two distinct objects are found similar in some aspects. That kind of insight is one of our goals, which directly differentiate our method from the local-preserving mappings. Besides, as stated in the previous facial image case, our method is also helpful in interpreting a very-high-dimensional dataset.

In the case studies, we only demonstrate the method in some small datasets. Nevertheless, there is no substantial difficulty in applying the method to larger datasets with more dimensions. But the dimensional analysis will have to handle a large bunch of dimensions at each time, e.g. a pixel area in the image. Also, only a group of dimensions can have clear semantics in that case. We build our method upon linear projections, since they are commonly used, easy to interpret and efficient to compute. Nevertheless, the focus-based framework, either point-based distortion reduction or group-based relationship enhancement, is also applicable to non-linear projections. But we'll have to conduct dimensional analysis with the help of other views. The computation could also be more complex and needs numerical approaches.

We only define three kinds of featured projections based on a direct division of the distance matrix. In this work, we do not consider relationships between two focuses, since much works already exist focusing on class separation tasks~\cite{DBLP:conf/ieeevast/ChooLKP10}~\cite{DBLP:journals/cgf/SedlmairTMT12}. The three features may be simple, but they are the most common analytic targets, and could be very helpful in the analysis. Besides, our framework is totally compatible with more complex metrics. Any kind of local feature can be captured, as long as it can be described in the form of data distances. As a future work, we would like to extend our method by including other types of projections, as well as more specific feature metrics.

\note{dimension suggestion may be not so incorrect, but still helpful}