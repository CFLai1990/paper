\section{Discussion}
\label{section:discussion}
\note{
\col{Outline for discussion:
\begin{enumerate}[(1)]
\item Extension: other featured projections, non-linear projections
\item Impact of data scale: increased dimensionality or data size, try a larger dataset with higher dimensionality
\item Comparison: local-preserving algorithms, projections for classification
\item Problems: lack of navigation, overlapping dimensions in a projection
\end{enumerate}
}
}
In this work, we propose to customize linear projections, to facilitate analyses regarding the user-specified local data. In Section~\ref{section:casestudy}, we prove the effectiveness of our method in revealing hidden relationships and reducing distortions. However, there are still flaws in the proposed method, which we would like to discuss in this section. After that, we illustrate the relationships and differences between our method and some state of art machine learning techniques. At last, we show that our method can be extended into a more general methodology.

The first flaw we'd like to mention, is the lack of effective navigation in the exploration. Dimension weights are like the steering wheel. They provide good controls to fine-tune a projection, though the tuning is often aimless. In our method, the steering wheel is the focus with its features. The exploration inevitably jumps between diverse projections when the focus or metric is updated. Even supplied with animations, users may still get confused when facing with the abrupt changes. Besides, we do not reserve historical images unless they are added in the focus list. Some intermediate results could be lost if they are missed by mistake. A proper way to remedy this flaw, is to provide dimensional controls as well as the navigation history. The second flaw is about the subspace suggestion. At present, we rank all dimensions according to their weights in the featured projection. However, dimensions vectors tend to overlap in the 2D plane. Two dimensions will cancel out each other if projected with opposite directions. Their weights are thus inaccurate. This problem becomes more severe when dimensions increase. A reasonable substitute is the rank-by-feature technique. We can isolate each dimension, and examine its feature strength to decide the weighting. It could provide more accurate estimations.

In the machine learning field, the commonly studied data use to have huge amounts of dimensions, while each dimension alone is not so meaningful. The face images are a classic example. In this case, the data can be assumed to lie on a low-dimensional manifold, where neighborhood relationships are more important than dimensional information. In this work, we also seek to preserve data locality, but we decide to maintain dimensional context by using linear projections. That's because, in a more general case, it's of interest how the data behave in a subspace. It's quite an interesting finding if two distinct objects are found similar in some aspects. That kind of insight is one of our goals, which directly differentiate our method from the local-preserving mappings.

In the case studies, we only demonstrate the method in some small datasets. Nevertheless, there is no substantial difficulty in applying the method to larger datasets with more dimensions. But it's unlikely for a few dimensions to dominate some feature in that circumstance. The subspace suggestion will become suboptimal. We build our method upon linear projections, since they are commonly used, easy to interpret and efficient to compute. Nevertheless, the focus-based framework, either point-based distortion reduction or group-based relationship enhancement, is also applicable to non-linear projections. But the computation could be more complex and needs numerical approaches. On the other hand, we only define three kinds of featured projections based on a direct division of the distance matrix. In this work, we do not consider relationships between two focuses, since much works already exist focusing on class separation tasks~\cite{DBLP:conf/ieeevast/ChooLKP10}~\cite{DBLP:journals/cgf/SedlmairTMT12}. The three rules may be simple, but our framework is totally compatible with more complex metrics. Any kind of local feature can be captured, as long as it can be described in the form of data distances. As a future work, we would like to extend our method by including other types of projections, as well as more specific feature metrics.

\note{dimension suggestion may be not so incorrect, but still helpful}